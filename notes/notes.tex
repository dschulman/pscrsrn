\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{fit,positioning}

\tikzstyle{block} = [draw, rectangle, align=center]
\tikzstyle{op} = [draw, inner sep=1pt, circle]

\newcommand{\real}{\mathbb{R}}

\title{Physiological Signal Classification with Recursive Sequence Reduction Networks}
\author{Daniel Schulman}

\begin{document}
\maketitle

\section{Introduction}

Classification of physiological signals, such as electromyography (EMG), electroencephalography (EEG), and electrocardiography (ECG), is a common task in applications of artificial intelligence and machine learning to healthcare.
In the last few years, deep learning models have become state-of-the-art in many physiological signal classification tasks, in several cases achieving performance superior to human experts.

Physiological signals differ from many of the domains that deep learning models were originally developed on, such as computer vision.
They typically have fairly high temporal resolution, so that even brief recordings of signals have many more samples than other types of sequential data (e.g. natural language).
Signal length often varies, and models that are designed around data with fixed dimensionality (e.g. CNNs) can be awkward.

In this paper, we introduce a novel approach --- a \emph{recursive sequence reduction network} (RSRNs) --- that provides a simple baseline approach that is well-suited to physiological signal classification tasks.
RSRNs handle long sequences and varying-length sequence, and can deliver competitive performance with relatively small models, which may make usage feasible on edge devices with limited memory or computational power.

We first introduce RSRNs, then present experiments demonstrating performance on a set of benchmark tasks.
We compare and contrast RSRNs to other models, and we discuss applications where RSRNs may be suitable.

\section{Recursive Sequence Reduction Networks}

One way we can understand a sequence classification task is as learning a transformation from a variable-dimension input (a sequence) to a fixed-dimension output (e.g. a vector of class probabilities).
We consider a function $f(x,\theta)$ that operates on sequences with a fixed number of features ($M$) but arbitrary length.
If $x \in \real^{M \times N}$, then $f(x, \theta) \in \real^{M \times N'}$, where $1 \leq N' \leq N/K$ for some $K \in \mathbb{Z}$, $K \geq 2$.
That is, $f$ reduces a sequence of arbitrary length ($N$) to a sequence that is a fraction of the length.
We can apply $f$ recursively to reduce a sequence to a fixed-dimension feature vector.

\begin{equation}
    F(x, \theta) =
    \begin{cases}
        x &\text{if } x \in \real^{M \times 1} \\
        F(f(x, \theta), \theta) &\text{otherwise}
    \end{cases}
\end{equation}

In our RSRNs, we model $f$ with a convolutional network; specifically, a variant of a resnet, using strided convolutions to reduce sequence length at each step.
However, in typical usage of a resnet-style network, we would stack multiple layers to create a deep network.
Here, we simply apply the \emph{same} layers (i.e. sharing weights) repeatedly until the sequence is reduced.

\begin{figure}
\centering
\begin{tikzpicture}[node distance=12pt]
\node (input) [coordinate] {};
\node (split) [fill, circle, inner sep=0pt, minimum size=4pt, above=of input] {};
\draw [->] (input) -- (split);
\node (conv1r) [block, above=of split] {Conv\\width=5\\stride=2};
\draw [->] (split) -- (conv1r);
\node (conv1l) [block, left=of conv1r] {Conv\\width=5\\stride=2};
\draw [->] (split) -| (conv1l);
\node (ln1) [block, above=of conv1r] {Layer Norm};
\draw [->] (conv1r) -- (ln1);
\node (relu1) [block, above=of ln1] {ReLU};
\draw [->] (ln1) -- (relu1);
\node (conv2) [block, above=of relu1] {Conv\\width=5\\stride=1};
\draw [->] (relu1) -- (conv2);
\node (ln2) [block, above=of conv2] {Layer Norm};
\draw [->] (conv2) -- (ln2);
\node (relu2) [block, above=of ln2] {ReLU};
\draw [->] (ln2) -- (relu2);
\node (sum) [op, above=of relu2] {$+$};
\draw [->] (conv1l) |- (sum);
\draw [->] (relu2) -- (sum);
\node (output) [coordinate, above=of sum] {};
\draw [->] (sum) -- (output);
\end{tikzpicture}
\caption{A Basic Block}
\label{fig:block}
\end{figure}

Our RSRN is a sequence of one or more basic blocks, where the number of blocks is a hyperparameter.
A basic block (Figure~\ref{fig:block}) is a series of two convolutions, with layer normalization and rectified linear activation, summed with a residual connection, which has its own convolution.
All convolutions use the same number of channels; we call this the ``hidden state size'', as in RNNs.
The first convolution is strided, reducing sequence length.
This differs in a few details from a block in a typical resnet.
First, we use layer normalization rather than batch normalization, as layer normalization is appropriate for a recursively-applied model.
Second, a typical resnet uses a width-one convolution on the residual connection; we must use a strided convolution here (to keep the sequence length the same through both paths), and a width-one strided convolution would discard some inputs.

\begin{figure}
\centering
\begin{tikzpicture}[node distance=12pt]
\node (input) [coordinate] {};
\node (inconv) [block, above=of input] {Conv\\width=7\\stride=4};
\draw [->] (input) -- (inconv);
\node (inln) [block, above=of inconv] {Layer Norm};
\draw [->] (inconv) -- (inln);
\node (inrelu) [block, above=of inln] {ReLU};
\draw [->] (inln) -- (inrelu);
\node (rin) [coordinate, above=of inrelu] {};
\draw (inrelu) -- (rin);
\node (block1) [block, above=of rin] {Block};
\draw [->] (rin) -- (block1);
\node (block2) [block, above=of block1] {Block};
\draw [->] (block1) -- (block2);
\node (rout) [coordinate, above=of block2] {};
\draw (block2) -- (rout);
\node (recur1) [coordinate, right=3em of rout] {};
\draw (rout) -- (recur1);
\node (recur2) [coordinate, right=3em of rin] {};
\draw (recur1) -- (recur2);
\draw [->] (recur2) -- (rin);
\node (dummy) [coordinate, left=2em of rout] {};
\node [
    draw, dashed, rectangle,
    fit=(rin) (rout) (block1) (block2) (recur1) (recur2) (dummy),
    label=left:RSRN
] {};
\node (outlin1) [block, above=of rout] {Linear};
\draw [->] (rout) -- (outlin1);
\node (outrelu) [block, above=of outlin1] {ReLU};
\draw [->] (outlin1) -- (outrelu);
\node (outlin2) [block, above=of outrelu] {Linear};
\draw [->] (outrelu) -- (outlin2);
\node (output) [coordinate, above=of outlin2] {};
\draw [->] (outlin2) -- (output);
\end{tikzpicture}
\caption{A RSRN Model for Sequence Classification}
\label{fig:model}
\end{figure}

Figure~\ref{fig:model} shows a simple and generic physiological sequence classification model using an RSRN.
First, input is passed through a convolutional layer which upsamples along the feature dimension (as the hidden state size of the RSRN will typically be much higher than the input feature dimension).
Our experiments show that a fairly wide strided convolution works well here.
We conjecture that it is useful to have some downsampling along the temporal dimension, where physiological signals are typically high resolution.

An RSRN is then used to reduce from an arbitrary-length sequence to a single hidden state.
Finally, a shallow fully-connected network produces outputs.

\section{Experiments}

\subsection{Tasks}

\subsection{Methods}

\subsection{Results}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig_train_loss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig_val_loss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig_accuracy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig_auc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig_f1}
\end{figure}

\end{document}